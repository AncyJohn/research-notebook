{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Feature Engineering Philosophy (Very Important)\n\nEvery feature must answer:\n\n“What information am I adding that the model didn’t already have?”\n\nWe’ll focus on classic Titanic signals that are:\n\nInterpretable\n\nLow-risk\n\nWidely accepted","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2026-01-13T04:51:04.112579Z","iopub.execute_input":"2026-01-13T04:51:04.112800Z","iopub.status.idle":"2026-01-13T04:51:08.421397Z","shell.execute_reply.started":"2026-01-13T04:51:04.112780Z","shell.execute_reply":"2026-01-13T04:51:08.420377Z"}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:01:19.648400Z","iopub.execute_input":"2026-01-13T10:01:19.648666Z","iopub.status.idle":"2026-01-13T10:01:20.964739Z","shell.execute_reply.started":"2026-01-13T10:01:19.648648Z","shell.execute_reply":"2026-01-13T10:01:20.963828Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load Data\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:01:24.410594Z","iopub.execute_input":"2026-01-13T10:01:24.411044Z","iopub.status.idle":"2026-01-13T10:01:24.449854Z","shell.execute_reply.started":"2026-01-13T10:01:24.411012Z","shell.execute_reply":"2026-01-13T10:01:24.449132Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Feature engineering\n\n# 1. Family Size: sum of siblings/spouses and parents/children + yourself\ntrain_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\n\n# 2. Is Alone: 1 if the passenger has no family on board, 0 otherwise\ntrain_df[\"IsAlone\"] = (train_df[\"FamilySize\"] == 1).astype(int)\n\n# 3. Title from Name: Extract text between the comma and period\n# This uses a regular expression to pull titles like \"Mr\", \"Mrs\", \"Master\", etc.\ntrain_df[\"Title\"] = train_df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\") #(..)->capture group [..] -> The logic\ntrain_df[\"Title\"] = train_df[\"Title\"].str.strip()\n\n# Optional: Map rare titles to a 'Rare' category to help the model generalize\n# This prevents overfitting on titles that only appear once or twice\nrare_titles = ['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'the Countess', 'Jonkheer']\ntrain_df[\"Title\"] = train_df[\"Title\"].replace(rare_titles, 'Rare')\ntrain_df[\"Title\"] = train_df[\"Title\"].replace(['Mlle', 'Ms'], 'Miss')\ntrain_df[\"Title\"] = train_df[\"Title\"].replace('Mme', 'Mrs')\n\n# Drop columns that are no longer needed (since we extracted their info)\n# This prevents the model from trying to process the raw 'Name' string\ntrain_df.drop(['Name', 'SibSp', 'Parch'], axis=1, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:25:34.996853Z","iopub.execute_input":"2026-01-13T10:25:34.997127Z","iopub.status.idle":"2026-01-13T10:25:35.016769Z","shell.execute_reply.started":"2026-01-13T10:25:34.997102Z","shell.execute_reply":"2026-01-13T10:25:35.015488Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define target & features\nX = train_df.drop(columns=[\"Survived\"])\ny = train_df[\"Survived\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:27:06.697470Z","iopub.execute_input":"2026-01-13T10:27:06.697724Z","iopub.status.idle":"2026-01-13T10:27:06.703010Z","shell.execute_reply.started":"2026-01-13T10:27:06.697704Z","shell.execute_reply":"2026-01-13T10:27:06.702266Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Train/Validation split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y # to preserve the original class distribution, since the Survival classes are imbalanced\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:27:11.029972Z","iopub.execute_input":"2026-01-13T10:27:11.030280Z","iopub.status.idle":"2026-01-13T10:27:11.039733Z","shell.execute_reply.started":"2026-01-13T10:27:11.030247Z","shell.execute_reply":"2026-01-13T10:27:11.038772Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Pre-processing\n#Identify column types\nnumeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncategorical_features = X_train.select_dtypes(include=[\"object\"]).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:27:15.383277Z","iopub.execute_input":"2026-01-13T10:27:15.383631Z","iopub.status.idle":"2026-01-13T10:27:15.390876Z","shell.execute_reply.started":"2026-01-13T10:27:15.383606Z","shell.execute_reply":"2026-01-13T10:27:15.389992Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Apply imputation & StandardScaler\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())  # to fix the Convergence Warning\n])\n\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:27:18.808766Z","iopub.execute_input":"2026-01-13T10:27:18.809049Z","iopub.status.idle":"2026-01-13T10:27:18.813600Z","shell.execute_reply.started":"2026-01-13T10:27:18.809029Z","shell.execute_reply":"2026-01-13T10:27:18.812760Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:27:21.903623Z","iopub.execute_input":"2026-01-13T10:27:21.903902Z","iopub.status.idle":"2026-01-13T10:27:21.907843Z","shell.execute_reply.started":"2026-01-13T10:27:21.903877Z","shell.execute_reply":"2026-01-13T10:27:21.907123Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Logistic regression model\nmodel = LogisticRegression(max_iter=1000) #sets a hard limit on the number of optimization steps\n\n# Full pipeline\nclf = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"model\", model)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:27:24.995145Z","iopub.execute_input":"2026-01-13T10:27:24.995450Z","iopub.status.idle":"2026-01-13T10:27:24.999999Z","shell.execute_reply.started":"2026-01-13T10:27:24.995423Z","shell.execute_reply":"2026-01-13T10:27:24.999260Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cross-Validation\ncv = StratifiedKFold(\n    n_splits=5,\n    shuffle=True,\n    random_state=42\n)\n\n# Run Cross-Validation\nscores_fe = cross_val_score(\n    clf,\n    X,\n    y,\n    cv=cv,\n    scoring=\"accuracy\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:28:06.642519Z","iopub.execute_input":"2026-01-13T10:28:06.643362Z","iopub.status.idle":"2026-01-13T10:28:06.883592Z","shell.execute_reply.started":"2026-01-13T10:28:06.643271Z","shell.execute_reply":"2026-01-13T10:28:06.882965Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"scores_fe.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:30:22.340322Z","iopub.execute_input":"2026-01-13T10:30:22.340587Z","iopub.status.idle":"2026-01-13T10:30:22.346079Z","shell.execute_reply.started":"2026-01-13T10:30:22.340568Z","shell.execute_reply":"2026-01-13T10:30:22.345146Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"np.float64(0.8383780051471973)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"scores_fe.std()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T10:30:56.081859Z","iopub.execute_input":"2026-01-13T10:30:56.082186Z","iopub.status.idle":"2026-01-13T10:30:56.086880Z","shell.execute_reply.started":"2026-01-13T10:30:56.082142Z","shell.execute_reply":"2026-01-13T10:30:56.086330Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"np.float64(0.00668036552010111)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## Feature Engineering Results\n\nAdded features:\n- FamilySize\n- IsAlone\n- Title\n\nResults:\n- Baseline CV accuracy: 0.807 STD:0.033\n- Feature-engineered CV accuracy: 0.8384 std:0.007\n\nObservations:\n- Which features likely helped?\n     The Title: This is almost certainly the primary driver of your improvement. \"Title\" is a high-signal feature because it captures \"hidden\" interactions. For example, \"Master\" (young boys) had a very high survival rate, which a standard model might miss if it only looked at \"Male\" and \"Age\" separately.\n  \nIsAlone / FamilySize: These likely helped by simplifying the relationship between social groups and survival. Logistic Regression performs better when a complex relationship (like household dynamics) is converted into a clear binary signal like IsAlone.\n- Any increase in variance?\n      No, variance actually decreased.\nYour standard deviation (STD) dropped from 0.033 to 0.007.\nThis is a massive improvement. A lower STD means your model is much more consistent across different folds of data. It suggests that your new features are \"universally true\" across the dataset, rather than being \"lucky\" features that only work on certain rows.\n- Does the improvement seem robust?\n      Accuracy Increase: You gained ~3.1% in accuracy (0.807 \\(\\rightarrow \\) 0.838).Standard Deviation Check: Since your new accuracy (0.838) is more than one baseline standard deviation away from the old mean (\\(0.807+0.033=0.840\\), you are right at the edge, but the stability is the key), the improvement is likely a real predictive gain rather than random noise.The \"Reliability\" Win: The drop in STD to 0.007 is the most impressive part. In 2026 machine learning workflows, a model with slightly lower accuracy but a very low STD is often preferred over a \"spiky\" model because it is more predictable when deployed on truly unseen data.\n\nSummary Comparison\nMetric: Mean CV\tBaseline: 0.807\tEngineered: 0.8384\tStatu: Improved\nMetric: STD (Variance) Baseline: 0.033 Engineered: 0.0070\tStatu: Much More Stable","metadata":{}}]}