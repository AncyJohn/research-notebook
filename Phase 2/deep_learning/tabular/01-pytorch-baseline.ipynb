{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nSEED = 42\n\ndef set_reproducibility(seed):\n    # 1. Standard Python randomness\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    # 2. Modern NumPy (Generator-based)\n    # Use this 'rng' for any direct numpy random calls in your code\n    rng = np.random.default_rng(seed)\n    # Optional: still set legacy global seed for libraries that depend on it\n    np.random.seed(seed) \n\n    # 3. PyTorch (CPU and all GPUs)\n    torch.manual_seed(seed) # for CPU/GPU weight initialization\n    torch.cuda.manual_seed_all(seed) # ensures all GPUs are seeded if you use multiple. \n\n    # 4. Forcing Deterministic GPU Algorithms (Crucial for 2026)\n    # This ensures operations like convolutions are identical every run\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    return rng\n\n# Initialize your setup\nrng = set_reproducibility(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-22T02:25:06.617979Z","iopub.execute_input":"2026-01-22T02:25:06.618356Z","iopub.status.idle":"2026-01-22T02:25:06.633291Z","shell.execute_reply.started":"2026-01-22T02:25:06.618326Z","shell.execute_reply":"2026-01-22T02:25:06.632197Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"For a neural network project, it is best practice to seed all possible sources of randomness at once to ensure your weights and training paths are identical every time you run the script. Setting os.environ['PYTHONHASHSEED'] remains a standard part of \"all-in-one\" reproducibility scripts. There is a major technical limitation you must know: modifying this inside your script usually does nothing.\nPython reads this environment variable only once, at the very moment the interpreter starts. By the time your code reaches os.environ['PYTHONHASHSEED'] = ..., the interpreter has already initialized its hashing mechanism with a random value. \nHow to use it correctly\nIf you need 100% strict reproducibility, you must set the variable before running your script on Terminal/ Command Line:            \n\n'export PYTHONHASHSEED=42 && python my_script.py'\n","metadata":{}},{"cell_type":"code","source":"# Load data \ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# Define target & features\n#Identify column types\nnum_cols = train_df.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncat_cols = train_df.select_dtypes(include=[\"object\"]).columns\n\n# Convert to Tensors/Lists\n# Numerical data must be a Float Tensor for nanmean to work\nnumerical_tensor = torch.tensor(train_df[num_cols].values, dtype=torch.float32)\n# Categorical data must be a list of lists for the _build_mappings logic\ncategorical_data = train_df[cat_cols].astype(str).values.tolist()\n\n# Define target\ny_tensor = torch.tensor(train_df[\"Survived\"].values, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T02:26:02.381803Z","iopub.execute_input":"2026-01-22T02:26:02.382135Z","iopub.status.idle":"2026-01-22T02:26:02.432005Z","shell.execute_reply.started":"2026-01-22T02:26:02.382108Z","shell.execute_reply":"2026-01-22T02:26:02.431015Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"A Class is a way to group data (attributes) and actions (methods) together into one package. PyTorch uses classes for almost everything because machine learning models are stateful. Unlike a function that forgets everything once it finishes, a Class remembers things.\n\nself is a way to attach information to the class. cat_mapping=None is A setting that allows the class to either invent a new category-to-number (mapping) system or reuse an old one.\n\nThe underscore _ at the start of the name is a Python convention. It signals to other programmers: \"This is an internal helper tool. You don't need to call this yourself; the class will use it automatically during setup.\"\n","metadata":{}},{"cell_type":"markdown","source":"Docstrings act as a \"manual\" for anyone reading your code (including your future self). By writing [N, num_features], the programmer is saying: \"This class expects a 2D table. If you try to pass a 1D list or a 3D cube of data, the code will break.\"\n\nThe Docstring (\"\"\" ... \"\"\")\nA Docstring (Documentation String) is meant for the user of the code and the Python system itself. \nFormat: Wrapped in triple quotes (\"\"\") right under a class or function definition.\nBehavior: Python actually stores this text as a special attribute called __doc__. It is not ignored.\nPurpose: It explains how to use the class/function, what the inputs are, and what it returns.\nVisibility: You can see it without opening the file. You can access it by typing help(YourClassName) or hovering your mouse over the code in a modern editor.","metadata":{}},{"cell_type":"markdown","source":"understanding dim (short for dimension) is the single most important concept for manipulating data in PyTorch.\nWhen you see dim=0 in a function like torch.nanmean(data, dim=0), you are telling PyTorch which direction to look when performing a calculation on a table (tensor).\n\nThink of a standard data table (a 2D Tensor):\ndim=0 refers to the Rows (Vertical axis).\ndim=1 refers to the Columns (Horizontal axis).","metadata":{}},{"cell_type":"markdown","source":"In PyTorch logic, we prefer Label Encoding because:\nMemory: It keeps your data small (just one column of integers).\nEmbeddings: PyTorch has a powerful layer called nn.Embedding that takes these integers and turns them into \"concept vectors,\" allowing the model to learn that \"London\" and \"Paris\" are both European cities.","metadata":{}},{"cell_type":"code","source":"# user-defined class that inherits from a built-in PyTorch template called Dataset.\nclass PreprocessingDataset(Dataset):\n    \n    # Method definition (A function that belongs to a specific class)\n    def __init__(self, numerical_data, categorical_data, cat_mappings=None):\n\n        #Docstrings\n        \"\"\"\n        numerical_data: Tensor with NaNs [N, num_features]\n        categorical_data: List of lists or Array of strings [N, num_cats]\n        \"\"\"\n        # 1. IMPUTATION (Logical Mean Imputation)\n        # Calculate mean while ignoring NaNs\n        self.num_means = torch.nanmean(numerical_data, dim=0)\n        # Fill NaNs with the calculated mean\n        self.num_data = torch.where(torch.isnan(numerical_data), self.num_means, numerical_data)\n        \n        # 2. STANDARDIZATION (Z-Score)\n        self.num_std = torch.std(self.num_data, dim=0)\n        # Avoid division by zero for constant features\n        self.num_std[self.num_std == 0] = 1.0 # replace 0 with 1. Dividing by 1.0 doesn't change the value of the numerator\n        self.num_standardized = (self.num_data - self.num_means) / self.num_std\n        \n        # 3. CATEGORICAL ENCODING (Label Encoding)\n        self.cat_mappings = cat_mappings or self._build_mappings(categorical_data) # This line decides whether to create or reuse the mappings.\n        self.cat_encoded = self._encode(categorical_data)\n\n    # To implement label coding\n    def _build_mappings(self, data):\n        mappings = []\n        # This loop goes through your data column by column.\n        for col_idx in range(len(data[0])): # data[0] looks at the first row to count how many columns exist.\n            # set(...): It grabs every value in the column and throws away duplicates. If \"London\" appears 500 times, the set keeps it only once.\n            # list(...): It converts those unique items into a list.\n            # sorted(...): It puts them in alphabetical order. This is crucial for consistency—it ensures that \"London\" always gets the same number every time you run the code\n            unique_vals = sorted(list(set(row[col_idx] for row in data)))\n            # Creates a dictionary for the columns\n            # enumerate: Assigns a number to each item in your sorted list (0, 1, 2...). It creates a map like {\"London\": 0, \"New York\": 1, \"Paris\": 2}.\n            mappings.append({val: i for i, val in enumerate(unique_vals)})\n        return mappings\n\n    def _encode(self, data):\n        encoded = []\n        for row in data:\n            # the actual \"translation\n            encoded_row = [self.cat_mappings[i].get(val, 0) for i, val in enumerate(row)] # The default 0\n            encoded.append(encoded_row)\n        # After all rows are processed, the code turns the big list of numbers into a PyTorch Tensor.\n        return torch.tensor(encoded, dtype=torch.long) # In PyTorch, categorical labels must be integers (64-bit integers, specifically)\n        \n    # return a single number: the total number of rows in the data.\n    def __len__(self):\n        return len(self.num_standardized)\n\n    # to get a single row of data when given an index (like \"row #42\").\n    def __getitem__(self, idx):\n        return self.num_standardized[idx], self.cat_encoded[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T02:26:46.964045Z","iopub.execute_input":"2026-01-22T02:26:46.964408Z","iopub.status.idle":"2026-01-22T02:26:46.975829Z","shell.execute_reply.started":"2026-01-22T02:26:46.964367Z","shell.execute_reply":"2026-01-22T02:26:46.974899Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- EXECUTION ---\n\n# Initialize the cleaning class\npreprocessor = PreprocessingDataset(numerical_tensor, categorical_data)\n\n# --- STEP 1: Finalize X_np and y_np ---\n# num_standardized and cat_encoded are tensors from our previous cleaning steps\n# Combine standardized numbers and encoded categories into one matrix\nX_tensor = torch.cat([preprocessor.num_standardized, preprocessor.cat_encoded], dim=1)\n\nX_np = X_tensor.numpy()    # Instruction: End with numpy array\ny_np = y_tensor.numpy()    # Instruction: End with numpy array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T02:31:39.030821Z","iopub.execute_input":"2026-01-22T02:31:39.031165Z","iopub.status.idle":"2026-01-22T02:31:39.036818Z","shell.execute_reply.started":"2026-01-22T02:31:39.031136Z","shell.execute_reply":"2026-01-22T02:31:39.035743Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# PyTorch Dataset\nclass TitanicDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1) # Align shape for loss\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_ds = TitanicDataset(X_np, y_np)\n# creates a pipeline to feed the neural network\n# batch_size group data into sets of n samples at a time\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T02:42:02.245511Z","iopub.execute_input":"2026-01-22T02:42:02.245932Z","iopub.status.idle":"2026-01-22T02:42:02.255926Z","shell.execute_reply.started":"2026-01-22T02:42:02.245899Z","shell.execute_reply":"2026-01-22T02:42:02.254764Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Processing the entire dataset at once is often too large for your computer's memory (RAM/VRAM). Hence we group it into multiple samples.","metadata":{}},{"cell_type":"code","source":"# Model definition\nclass BaselineNet(nn.Module): # The physical structure of the brain (the neurons).\n    def __init__(self, input_size):\n        super(BaselineNet, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8),\n            nn.ReLU(),\n            nn.Linear(8, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x): # The thought process (data goes in, a decision comes out).\n        return self.net(x)\n\n# setup\nmodel = BaselineNet(input_size=X_np.shape[1])\ncriterion = nn.BCELoss() # Binary Cross Entropy for 0/1 prediction (how wrong was the decision?)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01) # The learning process (adjusting the brain so there is less regret next time).\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T03:04:47.165258Z","iopub.execute_input":"2026-01-22T03:04:47.165645Z","iopub.status.idle":"2026-01-22T03:04:51.105358Z","shell.execute_reply.started":"2026-01-22T03:04:47.165582Z","shell.execute_reply":"2026-01-22T03:04:51.104165Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"This is a Multilayer Perceptron (MLP). It uses a series of \"Linear\" layers to transform input data into a prediction.\nnn.Module: This is the base class for all neural networks in PyTorch. Inheriting from it gives your model the ability to track \"gradients\" (how to change its weights to get better).\nsuper().__init__(): This initializes the parent nn.Module. Without this, PyTorch won't be able to track your layers.\nnn.Sequential: This is a container that runs layers in order, like an assembly line.\nnn.Linear(input_size, 16): The \"Input Layer.\" It takes your features (Age, Sex, etc.) and expands them to 16 hidden neurons.\nnn.ReLU(): An activation function. It turns negative numbers into 0. This \"non-linearity\" is what allows the model to learn complex patterns instead of just simple lines.\nnn.Linear(16, 8): A \"Hidden Layer.\" It condenses the 16 features down to 8, looking for deeper patterns.\nnn.Linear(8, 1): The \"Output Layer.\" It condenses everything down to a single number.\nnn.Sigmoid(): This squashes the output number to be between 0 and 1. This is perfect for the Titanic dataset because we want a probability (e.g., \"There is a 0.85 chance this person survived\").\n\nIn PyTorch, you don't call .predict(). Instead, you define a forward method. When you do model(X), PyTorch automatically runs this method. It simply passes the data x through the self.net assembly line we defined in the setup.\n\ninput_size=X_np.shape[1]: This tells the first layer how many columns are in your data. shape[1] refers to the number of columns (features).\nnn.BCELoss(): Stands for Binary Cross Entropy. This is the standard \"scoring\" system for 0/1 problems.\nIf the model predicts 0.9 and the person survived (1), the loss is low.\nIf the model predicts 0.1 and the person survived (1), the loss is high.\ntorch.optim.SGD: The Stochastic Gradient Descent optimizer. This is the \"brain\" that updates the model's weights.\nmodel.parameters(): You tell the optimizer: \"These are the weights you are allowed to change.\"\nlr=0.01: The Learning Rate. This controls how big of a \"step\" the model takes when correcting an error. Too big, and it overshoots; too small, and it takes forever to learn.\nSummary Analogy\nBaselineNet: The physical structure of the brain (the neurons).\nforward: The thought process (data goes in, a decision comes out).\nBCELoss: The feeling of regret (how wrong was the decision?).\nSGD Optimizer: The learning process (adjusting the brain so there is less regret next time).","metadata":{}},{"cell_type":"code","source":"# TRAINING LOOP\n# Set the number of epochs (times the model sees the whole dataset)\nepochs = 20\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0  # Reset the \"accumulator\" at the start of each epoch\n    correct_predictions = 0\n    total_samples = 0\n    \n    for X_batch, y_batch in train_loader:\n        # 1. Reset the gradients (clear the chalkboard)\n        optimizer.zero_grad()\n        \n        # 2. Forward pass (the model makes guesses)\n        logits = model(X_batch)\n        \n        # 3. Calculate how wrong the guesses were\n        loss = criterion(logits, y_batch)\n        \n        # 4. Backward pass (find out who to blame for the error)\n        loss.backward()\n        \n        # 5. Optimization step (adjust the weights)\n        optimizer.step()\n        \n        # Add the batch loss to our running total\n        # We use .item() to get the number and avoid memory buildup\n        running_loss += loss.item()\n\n        # --- ACCURACY LOGIC ---\n        # 1. Convert probabilities to 0 or 1 (Threshold = 0.5)\n        # If probability >= 0.5, predicted = 1. Else 0.\n        predicted = (logits >= 0.5).float()\n        \n        # 2. Count how many matches the actual labels (y_batch)\n        correct_predictions += (predicted == y_batch).sum().item()\n        total_samples += y_batch.size(0)\n    \n    # Calculate the average loss for this entire epoch\n    avg_loss = running_loss / len(train_loader)\n    accuracy = (correct_predictions / total_samples) * 100\n    \n    # Print progress every epoch (or use an if-statement for every 5th epoch)\n    print(f\"Epoch [{epoch+1:02d}] | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n\nprint(\"\\nTraining Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T04:45:44.375564Z","iopub.execute_input":"2026-01-22T04:45:44.375945Z","iopub.status.idle":"2026-01-22T04:45:44.973168Z","shell.execute_reply.started":"2026-01-22T04:45:44.375916Z","shell.execute_reply":"2026-01-22T04:45:44.972134Z"}},"outputs":[{"name":"stdout","text":"Epoch [01] | Loss: 0.6332 | Accuracy: 65.66%\nEpoch [02] | Loss: 0.6369 | Accuracy: 66.11%\nEpoch [03] | Loss: 0.6258 | Accuracy: 67.12%\nEpoch [04] | Loss: 0.6189 | Accuracy: 66.33%\nEpoch [05] | Loss: 0.6265 | Accuracy: 66.22%\nEpoch [06] | Loss: 0.6215 | Accuracy: 66.22%\nEpoch [07] | Loss: 0.6208 | Accuracy: 66.55%\nEpoch [08] | Loss: 0.6252 | Accuracy: 65.10%\nEpoch [09] | Loss: 0.6315 | Accuracy: 66.33%\nEpoch [10] | Loss: 0.6265 | Accuracy: 66.78%\nEpoch [11] | Loss: 0.6242 | Accuracy: 67.79%\nEpoch [12] | Loss: 0.6178 | Accuracy: 66.67%\nEpoch [13] | Loss: 0.6305 | Accuracy: 65.21%\nEpoch [14] | Loss: 0.6239 | Accuracy: 67.12%\nEpoch [15] | Loss: 0.6341 | Accuracy: 66.22%\nEpoch [16] | Loss: 0.6237 | Accuracy: 65.66%\nEpoch [17] | Loss: 0.6226 | Accuracy: 66.67%\nEpoch [18] | Loss: 0.6171 | Accuracy: 67.23%\nEpoch [19] | Loss: 0.6247 | Accuracy: 66.89%\nEpoch [20] | Loss: 0.6182 | Accuracy: 65.21%\n\nTraining Complete!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"In PyTorch, models have two primary modes: Train and Eval (Evaluation).\nWhat it does: It tells the model, \"I am about to start training, so enable all behaviors necessary for learning.\"\nWhy it's needed: Some neural network components (like Dropout or Batch Normalization) behave differently during training than they do during testing.\nIn Train mode, these layers are active to help the model learn and generalize.\nIn Eval mode (model.eval()), these layers are \"frozen\" to ensure the model gives consistent, stable predictions.","metadata":{}},{"cell_type":"code","source":"# --- FINAL VERIFICATIONS ---\n# PRINT RESULTS\n# 1. Print Final Loss\nprint(f\"\\nFinal Average Loss: {avg_loss:.4f}\")\n\n# 2. Run a Forward Pass (on one batch)\nmodel.eval()  # Switch to evaluation mode\nwith torch.no_grad():  # Disable gradient tracking to save memory\n    first_batch_features, _ = next(iter(train_loader))\n    forward_output = model(first_batch_features)\n\n# 3. Confirm no NaNs\nnan_check = torch.isnan(forward_output).any().item()\nprint(f\"Forward Pass successful. Contains NaNs: {nan_check}\")\n\n# 4. Confirm Shapes\nprint(f\"Input Shape: {first_batch_features.shape}\")   # Expected: [32, Total_Features]\nprint(f\"Output Shape: {forward_output.shape}\")       # Expected: [32, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T04:19:03.641981Z","iopub.execute_input":"2026-01-22T04:19:03.642315Z","iopub.status.idle":"2026-01-22T04:19:03.653448Z","shell.execute_reply.started":"2026-01-22T04:19:03.642288Z","shell.execute_reply":"2026-01-22T04:19:03.652358Z"}},"outputs":[{"name":"stdout","text":"\nFinal Average Loss: 0.6226\nForward Pass successful. Contains NaNs: False\nInput Shape: torch.Size([32, 12])\nOutput Shape: torch.Size([32, 1])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(X_np.shape[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T04:21:36.023585Z","iopub.execute_input":"2026-01-22T04:21:36.024120Z","iopub.status.idle":"2026-01-22T04:21:36.031005Z","shell.execute_reply.started":"2026-01-22T04:21:36.024079Z","shell.execute_reply":"2026-01-22T04:21:36.029418Z"}},"outputs":[{"name":"stdout","text":"12\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Reflections\nEven without tuning:\n\nLoss went down\n\nThe model learned something\n\nBut:\n\nIt didn’t obviously beat logistic regression\n\nIt felt fragile\n\nSmall changes caused big behavior shifts\n\nThat’s not a bug.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}