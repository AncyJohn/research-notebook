{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2442c4-14f2-40c3-8fb6-b5e9a7d0c305",
   "metadata": {},
   "source": [
    "Seeing a linear model fail made the need for deep learning feel obvious rather than theoretical.\n",
    "\n",
    "You manually added non-linearity (polynomials)\n",
    "\n",
    "The model learns non-linearity automatically; That’s deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7cfa4-cf4d-428d-b553-8649dc82d946",
   "metadata": {},
   "source": [
    "# First Neural Network Experiment\n",
    "\n",
    "## Research Question\n",
    "Can a neural network learn a non-linear relationship without manual feature engineering?\n",
    "\n",
    "## Hypothesis\n",
    "A neural network with non-linear activation functions can model non-linear data effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c870b5-9553-4fa8-a81a-6350ea94369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b0a7c3-8bde-4fe4-9390-e875c9150e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "X = rng.random((200, 1)) * 5\n",
    "y = X.squeeze()**2 + rng.standard_normal(200) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1dc0fd-6089-42ac-8811-5aeadcb0d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077c8af2-8a3b-4ed7-882d-87c740f62ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f054646-b914-4aa4-9992-869442ec7f1d",
   "metadata": {},
   "source": [
    "This code converts a data structure (like a NumPy array or Python list) into a formatted PyTorch Tensor suitable for a neural network. It performs three critical operations in one line:\n",
    "\n",
    "torch.tensor(y_train, ...)\n",
    "This creates a PyTorch tensor from your raw data. Tensors are the fundamental data structures in PyTorch, similar to NumPy arrays but optimized for GPU acceleration and automatic differentiation (calculating gradients).\n",
    "\n",
    "dtype=torch.float32\n",
    "This explicitly sets the data type to 32-bit floating point. \n",
    "Why it's necessary: PyTorch defaults to float32 for most operations. If your raw data is in integers or float64 (the default for Python and NumPy), many neural network layers (like nn.Linear) will throw a \"type mismatch\" error because they expect float32.\n",
    "Performance: float32 provides the ideal balance between numerical precision and computational speed on modern GPUs.\n",
    "\n",
    ".view(-1, 1) This reshapes the tensor into a 2D column vector. The -1 (Infer Dimension): Tells PyTorch to automatically calculate the number of rows based on the total number of elements in the tensor.The 1: Explicitly sets the second dimension to 1 column.Why it's necessary: PyTorch loss functions (like MSELoss) and linear layers expect the target variable (\\(y\\)) to be 2D, with the shape (number_of_samples, 1). If \\(y\\) is a 1D \"flat\" array, the model may produce incorrect results or dimension mismatch errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712bb536-ce53-40aa-9525-6ec3d9799404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network (Core Learning)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf9c25-7e01-4670-bded-9fe83af4c64f",
   "metadata": {},
   "source": [
    "This code defines a Neural Network architecture using PyTorch. In 2026, this remains the standard way to build custom models by inheriting from the nn.Module base class.\n",
    "\n",
    "Class Definition & Initialization\n",
    "class SimpleNN(nn.Module):\n",
    "Inheriting from nn.Module gives your class all the necessary tools to handle parameters, move to GPUs, and perform backpropagation.\n",
    "super().__init__() This line is mandatory. It initializes the internal PyTorch machinery within the parent nn.Module class so that your custom layers are tracked correctly. [1]\n",
    "\n",
    "nn.Sequential (The \"Container\")\n",
    "Instead of defining layers separately, nn.Sequential wraps them into a single object. Data flows through these layers in the exact order they are listed. [1, 2]\n",
    "nn.Linear(1, 16) (Input Layer):This is a \"Fully Connected\" (Dense) layer. It takes 1 input (your single feature \\(X\\)) and projects it into a 16-dimensional hidden space. This expansion allows the model to learn more complex patterns.\n",
    "nn.ReLU() (Activation Function):ReLU stands for Rectified Linear Unit. It replaces all negative values with zero. This is the most critical part: without this, your model is just a series of linear equations (a straight line). ReLU adds the \"non-linearity\" needed to fit curves.[3]\n",
    "nn.Linear(16, 1) (Output Layer):This compresses the 16 hidden signals back down to 1 single output (your prediction \\(y\\)).\n",
    "\n",
    "Input: 1 value (e.g., \\(X=2.5\\))\n",
    "Linear 1: Becomes 16 different weighted values.\n",
    "ReLU: Any negative values in those 16 are \"turned off\" (set to 0).\n",
    "Linear 2: Combines those 16 values into 1 final prediction (e.g., \\(y=11.2\\))\n",
    "\n",
    "First layer: learns features\n",
    "\n",
    "ReLU: introduces non-linearity\n",
    "\n",
    "Output layer: regression prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459a722a-82a8-43e0-b23d-c7304c894c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a8b5d-6613-46b2-99fc-4257befc1d34",
   "metadata": {},
   "source": [
    "This code snippet initializes the three core components of a PyTorch training pipeline: the architecture, the error metric, and the update mechanism.\n",
    "\n",
    "model = SimpleNN() \n",
    "Purpose: This creates an instance of a neural network class (which you must define beforehand).\n",
    "Function: It allocates the layers (weights and biases) in memory. The variable model now represents the specific mathematical function that will take input data and produce a prediction. \n",
    "\n",
    "criterion = nn.MSELoss() \n",
    "Purpose: Defines the Loss Function, which measures how \"wrong\" the model is.\n",
    "Function: MSELoss stands for Mean Squared Error. It calculates the average squared difference between the model's predictions and the actual target values. It is the standard choice for regression tasks.\n",
    "Documentation: PyTorch MSELoss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "Purpose: Defines the Optimization Algorithm used to update the model's weights to reduce the loss.\n",
    "Components:\n",
    "optim.Adam: An advanced algorithm that uses adaptive learning rates for each parameter, generally performing faster than standard Stochastic Gradient Descent (SGD).\n",
    "model.parameters(): Tells the optimizer which weights and biases it is allowed to modify during training.\n",
    "lr=0.01: Sets the Learning Rate. This determines the size of the \"step\" the optimizer takes. At 0.01, the model adjusts its weights by 1% of the calculated gradient magnitude each step.\n",
    "Documentation: PyTorch Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f663be74-47f7-4726-925f-5588efda3779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 129.4090\n",
      "Epoch 100, Loss: 6.7439\n",
      "Epoch 200, Loss: 4.3391\n",
      "Epoch 300, Loss: 4.1285\n",
      "Epoch 400, Loss: 4.0952\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_t)\n",
    "    loss = criterion(outputs, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96746e25-d541-4c89-a6dd-9861620a8c76",
   "metadata": {},
   "source": [
    "This loop is the heart of deep learning. This code is the standard training loop in PyTorch. It repeats a specific set of steps to help the model learn from your data over multiple iterations.\n",
    "\n",
    "Core Loop Structure\n",
    "epochs = 500: An epoch is one full pass through your entire training dataset. This line specifies that the model will see and learn from the data 500 times.\n",
    "for epoch in range(epochs):: This initiates the loop, running the training steps 500 times in sequence. \n",
    "\n",
    "The 5 Essential Steps of Training\n",
    "Inside the loop, every iteration follows this mandatory order:\n",
    "optimizer.zero_grad() (#Reset): In PyTorch, gradients are accumulated (added together) by default. This command clears out the old gradients from the previous pass so you start each epoch with a clean slate.\n",
    "outputs = model(X_train_t) (#Forward Pass): The model takes your training data (X_train_t) and makes a prediction based on its current internal weights.\n",
    "loss = criterion(outputs, y_train_t) (#Calculate Error): The criterion compares the model's predictions (outputs) against the actual correct answers (y_train_t) to see how much error it made.\n",
    "loss.backward() (#Backpropagation): This is the \"math\" step. It calculates exactly how much each weight in the model contributed to the error.\n",
    "optimizer.step() (#Update Weights): The optimizer uses the information from the backward pass to slightly adjust the model's weights in the direction that will reduce the error next time.\n",
    "\n",
    "Progress Monitoring\n",
    "if epoch % 100 == 0:: This logic ensures you don't clutter your screen; it prints a status update every 100 epochs.\n",
    "loss.item():.4f: loss.item() extracts the error value from a PyTorch tensor and converts it into a standard Python number. The .4f formats it to 4 decimal places for readability. \n",
    "Are you seeing the loss value go down as it prints? If it stays high or goes up, we may need to adjust your learning rate (lr).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d1e986-9924-411f-be13-7b9bedc098b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2512125968933105"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_t)\n",
    "    mse_nn = mean_squared_error(\n",
    "        y_test_t.numpy(), predictions.numpy()\n",
    "    )\n",
    "\n",
    "mse_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854786a7-91df-47f5-9083-1c82898805e3",
   "metadata": {},
   "source": [
    "This code performs a \"clean\" evaluation of your model on the test dataset to measure how well it generalizes to unseen data.\n",
    "\n",
    "model.eval()\n",
    "Purpose: Switches the model from Training Mode to Evaluation Mode.\n",
    "Effect: This disables specific layers that should only be active during training, such as Dropout (which randomly shuts off neurons) and \n",
    "Batch Normalization (which uses running statistics instead of current batch data). This ensures your predictions are consistent and deterministic.\n",
    "\n",
    "with torch.no_grad():\n",
    "Purpose: Temporarily deactivates the Autograd engine.\n",
    "Effect: Since you are only making predictions and not updating weights, you don't need to track gradients. \n",
    "This drastically reduces memory consumption and speeds up computation by not storing intermediate values required for backpropagation.\n",
    "\n",
    "Making Predictions and Converting to NumPy\n",
    "predictions = model(X_test_t): Pass the test data through the model to get its final predictions.\n",
    ".numpy(): Most standard evaluation libraries (like Scikit-Learn) cannot read PyTorch Tensors directly. You must convert both your actual values (y_test_t) and your model's predictions back into standard NumPy arrays for calculation.\n",
    "\n",
    "mean_squared_error(...)\n",
    "Purpose: Calculates the final score for your model.\n",
    "Function: It computes the average squared difference between the true test values and the predicted values. A lower value indicates a more accurate model.\n",
    "\n",
    "Tip for 2026: If your loss was very high during training, checking mse_nn here will tell you if the model is actually \"learning\" or just memorizing the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4dc8e-1ed2-4ea9-9549-9ebc7dc4e100",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "- Linear Regression: High error\n",
    "- Polynomial Regression: Low error\n",
    "- Neural Network: Comparable or better error\n",
    "\n",
    "### Interpretation\n",
    "The neural network learned non-linear patterns automatically without manual feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a573a4-33da-4570-a8ea-4a76edb95b72",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- Neural networks are flexible function approximators\n",
    "- Non-linearity is learned via activation functions\n",
    "- This feels like a natural extension of earlier models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a8d72-f391-4535-9c35-03710056226a",
   "metadata": {},
   "source": [
    "In 2026, it is widely recognized that while Neural Networks (NN) are more flexible, Polynomial Regression often outperforms them on test data when working with smaller, structured, or low-dimensional datasets. \n",
    "If my polynomial model's test error is lower, it indicates that the polynomial approach is a better \"fit\" for the specific complexity of my data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
