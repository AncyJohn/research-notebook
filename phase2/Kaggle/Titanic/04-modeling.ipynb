{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:56:28.445749Z","iopub.execute_input":"2026-01-13T21:56:28.446277Z","iopub.status.idle":"2026-01-13T21:56:29.972113Z","shell.execute_reply.started":"2026-01-13T21:56:28.446249Z","shell.execute_reply":"2026-01-13T21:56:29.970219Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load Data\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:56:53.655777Z","iopub.execute_input":"2026-01-13T21:56:53.656736Z","iopub.status.idle":"2026-01-13T21:56:53.694719Z","shell.execute_reply.started":"2026-01-13T21:56:53.656704Z","shell.execute_reply":"2026-01-13T21:56:53.693687Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#Feature engineering\n\n# 1. Family Size: sum of siblings/spouses and parents/children + yourself\ntrain_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\n\n# 2. Is Alone: 1 if the passenger has no family on board, 0 otherwise\ntrain_df[\"IsAlone\"] = (train_df[\"FamilySize\"] == 1).astype(int)\n\n# 3. Title from Name: Extract text between the comma and period\n# This uses a regular expression to pull titles like \"Mr\", \"Mrs\", \"Master\", etc.\ntrain_df[\"Title\"] = train_df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\") #(..)->capture group [..] -> The logic\ntrain_df[\"Title\"] = train_df[\"Title\"].str.strip()\n\n# Optional: Map rare titles to a 'Rare' category to help the model generalize\n# This prevents overfitting on titles that only appear once or twice\nrare_titles = ['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'the Countess', 'Jonkheer']\ntrain_df[\"Title\"] = train_df[\"Title\"].replace(rare_titles, 'Rare')\ntrain_df[\"Title\"] = train_df[\"Title\"].replace(['Mlle', 'Ms'], 'Miss')\ntrain_df[\"Title\"] = train_df[\"Title\"].replace('Mme', 'Mrs')\n\n# Drop columns that are no longer needed (since we extracted their info)\n# This prevents the model from trying to process the raw 'Name' string\ntrain_df.drop(['Name', 'SibSp', 'Parch'], axis=1, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:56:58.508569Z","iopub.execute_input":"2026-01-13T21:56:58.508961Z","iopub.status.idle":"2026-01-13T21:56:58.544693Z","shell.execute_reply.started":"2026-01-13T21:56:58.508934Z","shell.execute_reply":"2026-01-13T21:56:58.543368Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Define target & features\nX = train_df.drop(columns=[\"Survived\"])\ny = train_df[\"Survived\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:57:05.085181Z","iopub.execute_input":"2026-01-13T21:57:05.086100Z","iopub.status.idle":"2026-01-13T21:57:05.097119Z","shell.execute_reply.started":"2026-01-13T21:57:05.086045Z","shell.execute_reply":"2026-01-13T21:57:05.094830Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Train/Validation split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y # to preserve the original class distribution, since the Survival classes are imbalanced\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:57:11.473440Z","iopub.execute_input":"2026-01-13T21:57:11.474408Z","iopub.status.idle":"2026-01-13T21:57:11.488304Z","shell.execute_reply.started":"2026-01-13T21:57:11.474378Z","shell.execute_reply":"2026-01-13T21:57:11.487015Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Pre-processing\n#Identify column types\nnumeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncategorical_features = X_train.select_dtypes(include=[\"object\"]).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:57:14.949448Z","iopub.execute_input":"2026-01-13T21:57:14.950035Z","iopub.status.idle":"2026-01-13T21:57:14.958580Z","shell.execute_reply.started":"2026-01-13T21:57:14.950006Z","shell.execute_reply":"2026-01-13T21:57:14.957331Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Apply imputation & StandardScaler\nnumeric_transformer1 = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())  # to fix the Convergence Warning\n])\n\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:50:25.902124Z","iopub.execute_input":"2026-01-13T22:50:25.902663Z","iopub.status.idle":"2026-01-13T22:50:25.908589Z","shell.execute_reply.started":"2026-01-13T22:50:25.902635Z","shell.execute_reply":"2026-01-13T22:50:25.907654Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Preprocessing pipeline\npreprocessor1 = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer1, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:50:45.682008Z","iopub.execute_input":"2026-01-13T22:50:45.682494Z","iopub.status.idle":"2026-01-13T22:50:45.688478Z","shell.execute_reply.started":"2026-01-13T22:50:45.682470Z","shell.execute_reply":"2026-01-13T22:50:45.686949Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Cross-Validation\ncv = StratifiedKFold(\n    n_splits=5,\n    shuffle=True,\n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Ligistic Regressio","metadata":{}},{"cell_type":"code","source":"# Logistic regression model\nlg = LogisticRegression(max_iter=1000) #sets a hard limit on the number of optimization steps\n\n# Full pipeline\nlg_pipeline = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor1),\n        (\"model\", lg)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:51:03.749747Z","iopub.execute_input":"2026-01-13T22:51:03.750383Z","iopub.status.idle":"2026-01-13T22:51:03.755963Z","shell.execute_reply.started":"2026-01-13T22:51:03.750349Z","shell.execute_reply":"2026-01-13T22:51:03.754691Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# Tree based Models\nSince tree-based models like Gradient Boosting or Random Forests split data based on relative value rankings rather than absolute distances, they are invariant to feature scaling.","metadata":{}},{"cell_type":"code","source":"#Define transformers (Scaling removed for Tree-based models)\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n])\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:52:02.598202Z","iopub.execute_input":"2026-01-13T22:52:02.598548Z","iopub.status.idle":"2026-01-13T22:52:02.604641Z","shell.execute_reply.started":"2026-01-13T22:52:02.598516Z","shell.execute_reply":"2026-01-13T22:52:02.603593Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"2. Random Forest\n   Captures non-linear interactions","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    random_state=42,\n    n_jobs=-1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:05:12.235446Z","iopub.execute_input":"2026-01-13T22:05:12.236479Z","iopub.status.idle":"2026-01-13T22:05:12.527696Z","shell.execute_reply.started":"2026-01-13T22:05:12.236445Z","shell.execute_reply":"2026-01-13T22:05:12.526102Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Full pipeline\nrf_pipeline = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"model\", rf)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:52:23.871264Z","iopub.execute_input":"2026-01-13T22:52:23.871764Z","iopub.status.idle":"2026-01-13T22:52:23.877564Z","shell.execute_reply.started":"2026-01-13T22:52:23.871739Z","shell.execute_reply":"2026-01-13T22:52:23.876019Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"3. Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb_classifier = GradientBoostingClassifier(\n                                            n_estimators=100, \n                                            learning_rate=0.1, \n                                            max_depth=3, \n                                            random_state=42)\n\n# Full pipeline\ngb_pipeline = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"model\", gb_classifier)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:52:41.401697Z","iopub.execute_input":"2026-01-13T22:52:41.402732Z","iopub.status.idle":"2026-01-13T22:52:41.407962Z","shell.execute_reply.started":"2026-01-13T22:52:41.402702Z","shell.execute_reply":"2026-01-13T22:52:41.406861Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"models = {\n    \"LogReg\": lg_pipeline,\n    \"RandomForest\": rf_pipeline,\n    \"GradientBoosting\": gb_pipeline\n}\n\nfor name, model in models.items():\n    scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:52:52.656957Z","iopub.execute_input":"2026-01-13T22:52:52.657261Z","iopub.status.idle":"2026-01-13T22:53:01.990065Z","shell.execute_reply.started":"2026-01-13T22:52:52.657232Z","shell.execute_reply":"2026-01-13T22:53:01.988738Z"}},"outputs":[{"name":"stdout","text":"LogReg: 0.8384 (+/- 0.0067)\nRandomForest: 0.8372 (+/- 0.0143)\nGradientBoosting: 0.8271 (+/- 0.0085)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## Model Comparison Results\n\nModels evaluated:\n- Logistic Regression\n- Random Forest\n- Gradient Boosting\n\nResults:\n- LogReg CV accuracy: 0.8384 (+/- 0.0067)\n- RF CV accuracy: 0.8372 (+/- 0.0143)\n- GB CV accuracy: 0.8271 (+/- 0.0085)\n\nObservations:\n- Which model performed best?\n  Logistic Regression (LogReg) is the best performer with a CV accuracy of 0.8384.\nIt numerically outperformed both Random Forest (0.8372) and Gradient Boosting (0.8271).\nThe fact that a simple linear model outperformed complex ensembles suggests your data may have a predominantly linear structure or a small sample size where complex models are prone to overfitting.\n\n- Did variance increase?\n  Yes, variance increased significantly as model complexity increased:\nLogReg has the lowest variance (+/- 0.0067), indicating high stability across different data folds.\nRandom Forest shows the highest variance (+/- 0.0143), more than double that of Logistic Regression. This suggests the RF model is highly sensitive to the specific training samples in each fold.\nGradient Boosting has moderate variance (+/- 0.0085) but the lowest overall accuracy, likely due to under-tuning or a lack of complex non-linear patterns to exploit.\n\n\n- Does the performance gain justify complexity?\n    Does the performance gain justify complexity?\nNo. In this specific case, there is no performance gain to justify the added complexity.\nNegative Gain: Moving from LogReg to GB actually results in a ~1.1% decrease in accuracy while increasing computational costs.\nOperational Efficiency: Logistic Regression is significantly faster for inference and easier to interpret.\nStability: LogReg offers the most \"reliable\" predictions given its superior accuracy and lowest standard deviation.\n","metadata":{}}]}